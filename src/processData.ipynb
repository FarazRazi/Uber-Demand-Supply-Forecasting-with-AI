{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to clean and pre-process the data\n",
    "# import data using pandas\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_map finished\n",
      "cluster_map.head(): \n",
      "                         region_hash  region_id\n",
      "0  90c5a34f06ac86aee0fd70e2adce7d8a          1\n",
      "1  f2c8c4bb99e6377d21de71275afd6cd2          2\n",
      "2  58c7a4888306d8ff3a641d1c0feccbe3          3\n",
      "3  b26a240205c852804ff8758628c0a86a          4\n",
      "4  4b9e4cf2fbdc8281b8a1f9f12b80ce4d          5\n"
     ]
    }
   ],
   "source": [
    "# label the cluster map\n",
    "# labels:\n",
    "# region_hash, region_id\n",
    "\n",
    "columns = ['region_hash', 'region_id']\n",
    "# read the cluster map\n",
    "cluster_map = pd.read_csv('../dataset/training_data/cluster_map/cluster_map', sep='\\t', on_bad_lines='skip', header=None, names=columns)\n",
    "print('cluster_map finished')\n",
    "\n",
    "print('cluster_map.head(): \\n', cluster_map.head())\n",
    "\n",
    "\n",
    "cluster_map.to_csv('../dataset/labeledData/cluster_map.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-01\n",
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-02\n",
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-03\n",
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-04\n",
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-05\n",
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-06\n",
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-07\n",
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-08\n",
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-09\n",
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-10\n",
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-11\n",
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-12\n",
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-13\n",
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-14\n",
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-15\n",
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-16\n",
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-17\n",
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-18\n",
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-19\n",
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-20\n",
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-21\n",
      "orders_data finished\n"
     ]
    }
   ],
   "source": [
    "# label the orders data\n",
    "# labels:\n",
    "# order_id, driver_id, passenger_id, start_district_hash, dest_district_hash, price, time\n",
    "columns = ['order_id', 'driver_id', 'passenger_id', 'start_region_hash', 'dest_region_hash', 'price', 'time']\n",
    "\n",
    "# read the orders data\n",
    "orders_data = []\n",
    "for f in glob.glob('../dataset/training_data/order_data/order_data_*'):\n",
    "    # file name\n",
    "    print('filename: ', f)\n",
    "    df = pd.read_csv(f, sep='\\t', on_bad_lines='skip', header=None, names=columns)\n",
    "    orders_data.append(df)\n",
    "\n",
    "print('orders_data finished')\n",
    "orders_data = pd.concat(orders_data,  ignore_index=True)\n",
    "\n",
    "# print('orders_data.head(): ', orders_data.head())\n",
    "orders_data.to_csv('../dataset/labeledData/orders_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-01\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-02\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-03\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-04\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-05\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-06\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-07\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-08\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-09\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-10\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-11\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-12\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-13\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-14\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-15\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-16\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-17\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-18\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-19\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-20\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-21\n",
      "weather_data finished\n"
     ]
    }
   ],
   "source": [
    "# label the weather data\n",
    "# labels:\n",
    "# time, weather, temperature, pm25\n",
    "columns = ['time', 'weather', 'temperature', 'pm25']\n",
    "\n",
    "# print('weather_data.head(): \\n', weather_data.head())\n",
    "\n",
    "\n",
    "# # read the weather data\n",
    "weather_data = []\n",
    "for f in glob.glob('../dataset/training_data/weather_data/weather_data_*'):\n",
    "    # file name\n",
    "    print('filename: ', f)\n",
    "    df = pd.read_csv(f, sep='\\t', on_bad_lines='skip', header=None, names=columns)\n",
    "    weather_data.append(df)\n",
    "\n",
    "print('weather_data finished')\n",
    "weather_data = pd.concat(weather_data, ignore_index=True)\n",
    "\n",
    "\n",
    "# weather_data.to_csv('../dataset/labeledData/weather_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled_poi_data.head():                          region_hash  poi_ids\n",
      "0  74c1c25f4b283fa74a5514307b0d0278   878555\n",
      "1  08f5b445ec6b29deba62e6fd8b0325a6    51128\n",
      "2  4b7f6f4e2bf237b6cc58f57142bea5c0   104248\n",
      "3  a814069db8d32f0fa6e188f41059c6e1   121844\n",
      "4  8316146a6f78cc6d9f113f0390859417    37599\n"
     ]
    }
   ],
   "source": [
    "# label the poi data\n",
    "# labels:\n",
    "# region_hash, poi_id \n",
    "# 1st column: district_hash\n",
    "# whole next column is: poi_id\n",
    "columns = ['region_hash', 'poi_id']\n",
    "\n",
    "\n",
    "\n",
    "# read the poi data\n",
    "poi_data = pd.read_csv('../dataset/training_data/poi_data/poi_data', sep='\\t', header=None, on_bad_lines='skip')\n",
    "\n",
    "# extract the district_hash column and the POI ID columns\n",
    "district_hash = poi_data.iloc[:, 0]\n",
    "poi_ids = poi_data.iloc[:, 1:]\n",
    "\n",
    "# combine all the POI IDs for each row into a list\n",
    "poi_ids_list = poi_ids.apply(lambda x: x.tolist(), axis=1)\n",
    "\n",
    "# combine the district_hash and poi_ids_list into a new DataFrame\n",
    "labeled_poi_data = pd.concat([district_hash, poi_ids_list], axis=1)\n",
    "labeled_poi_data.columns = ['region_hash', 'poi_ids']\n",
    "\n",
    "# print the result\n",
    "# print(labeled_poi_data.head())\n",
    "\n",
    "# updated list\n",
    "updated_list = []\n",
    "\n",
    "# convert the column of lists to a list of lists\n",
    "list_of_lists_poi_id = labeled_poi_data['poi_ids'].tolist()\n",
    "\n",
    "# poi format poi_id = class:numofFacilities\n",
    "# seperate numofFacilities from list_of_lists_poi_id and sum them up\n",
    "\n",
    "# for each list in list_of_lists_poi_id \n",
    "# change the list of poi_id to sum of numofFacilities \n",
    "for list in list_of_lists_poi_id:\n",
    "    sum = 0\n",
    "    for poi in list:\n",
    "        if(pd.isna(poi)==False):\n",
    "            poi = poi.split(':')\n",
    "            poi[1] = int(poi[1])\n",
    "            sum += poi[1]\n",
    "    updated_list.append(sum)\n",
    "\n",
    "# print(list_of_lists_poi_id)\n",
    "\n",
    "# change labeled_poi_data['poi_ids'] to list_of_lists_poi_id\n",
    "labeled_poi_data['poi_ids'] = updated_list\n",
    "\n",
    "print('labeled_poi_data.head(): ', labeled_poi_data.head())\n",
    "\n",
    "labeled_poi_data.to_csv('../dataset/labeledData/poi_data.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from the labeled data\n",
    "cluster_map = pd.read_csv('../dataset/labeledData/cluster_map.csv')\n",
    "orders_data = pd.read_csv('../dataset/labeledData/orders_data.csv')\n",
    "weather_data = pd.read_csv('../dataset/labeledData/weather_data.csv')\n",
    "poi_data = pd.read_csv('../dataset/labeledData/poi_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           order_id                         driver_id  \\\n",
      "0  97ebd0c6680f7c0535dbfdead6e51b4b  dd65fa250fca2833a3a8c16d2cf0457c   \n",
      "1  92c3ac9251cc9b5aab90b114a1e363be  c077e0297639edcb1df6189e8cda2c3d   \n",
      "2  abeefc3e2aec952468e2fd42a1649640  86dbc1b68de435957c61b5a523854b69   \n",
      "3  cb31d0be64cda3cc66b46617bf49a05c  4fadfa6eeaa694742de036dddf02b0c4   \n",
      "4  139d492189ae5a933122c098f63252b3                               NaN   \n",
      "\n",
      "                       passenger_id                 start_region_hash  \\\n",
      "0  ed180d7daf639d936f1aeae4f7fb482f  4725c39a5e5f4c188d382da3910b3f3f   \n",
      "1  191a180f0a262aff3267775c4fac8972  82cc4851f9e4faa4e54309f8bb73fd7c   \n",
      "2  7029e813bb3de8cc73a8615e2785070c  fff4e8465d1e12621bc361276b6217cf   \n",
      "3  21dc133ac68e4c07803d1c2f48988a83  4b7f6f4e2bf237b6cc58f57142bea5c0   \n",
      "4  26963cc76da2d8450d8f23fc357db987  fc34648599753c9e74ab238e9a4a07ad   \n",
      "\n",
      "                   dest_region_hash  price  time_slot  weekday  \n",
      "0  3e12208dd0be281c92a6ab57d9a6fb32   24.0         81        4  \n",
      "1  b05379ac3f9b7d99370d443cfd5dcc28    2.0         58        4  \n",
      "2  fff4e8465d1e12621bc361276b6217cf    9.0        110        4  \n",
      "3  4b7f6f4e2bf237b6cc58f57142bea5c0   11.0        133        4  \n",
      "4  87285a66236346350541b8815c5fae94    4.0        102        4  \n",
      "   weather  temperature  pm25  time_slot  weekday\n",
      "0        1          4.0   177          0        4\n",
      "1        1          3.0   177          0        4\n",
      "2        1          3.0   177          1        4\n",
      "3        1          3.0   177          1        4\n",
      "4        1          3.0   177          2        4\n"
     ]
    }
   ],
   "source": [
    "# map time to time slot\n",
    "# devide day in 10 min time slots (144 time slots)\n",
    "\n",
    "\n",
    "# convert time to datetime\n",
    "orders_data['time'] = pd.to_datetime(orders_data['time'])\n",
    "weather_data['time'] = pd.to_datetime(weather_data['time'])\n",
    "\n",
    "# map time to time slot\n",
    "orders_data['time_slot'] = orders_data['time'].dt.hour * 6 + orders_data['time'].dt.minute // 10\n",
    "weather_data['time_slot'] = weather_data['time'].dt.hour * 6 + weather_data['time'].dt.minute // 10\n",
    "\n",
    "# map time to time slot with weekday\n",
    "orders_data['weekday'] = orders_data['time'].dt.weekday\n",
    "weather_data['weekday'] = weather_data['time'].dt.weekday\n",
    "\n",
    "# remove the time column\n",
    "orders_data = orders_data.drop(['time'], axis=1)\n",
    "weather_data = weather_data.drop(['time'], axis=1)\n",
    "\n",
    "print(orders_data.head())\n",
    "print(weather_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  start_region_hash  time_slot  weekday  order_gap  demand  \\\n",
      "0  08232402614a9b48895cc3d0aeb0e9f2          0        0          1       1   \n",
      "1  08232402614a9b48895cc3d0aeb0e9f2          0        3          1       2   \n",
      "2  08232402614a9b48895cc3d0aeb0e9f2          0        4          5       5   \n",
      "3  08232402614a9b48895cc3d0aeb0e9f2          0        5          2       2   \n",
      "4  08232402614a9b48895cc3d0aeb0e9f2          0        6          3       5   \n",
      "\n",
      "   supply  \n",
      "0       0  \n",
      "1       1  \n",
      "2       0  \n",
      "3       0  \n",
      "4       2  \n"
     ]
    }
   ],
   "source": [
    "# group the orders data by time slot \n",
    "# aggregate count the number of orders where driver_id = NULL\n",
    "\n",
    "# this is supply demand deficit - order gap\n",
    "orders_data_grouped = orders_data[orders_data['driver_id'].isnull()].groupby(['start_region_hash','time_slot', 'weekday']).agg({'order_id': 'count'}).rename(columns={'order_id': 'order_gap'}).reset_index()\n",
    "\n",
    "# this is the total demand\n",
    "total_orders_grouped = orders_data.groupby(['start_region_hash','time_slot', 'weekday']).agg({'order_id': 'count'}).rename(columns={'order_id': 'demand'}).reset_index()\n",
    "\n",
    "# merge the two dataframes on the region, time slot and weekday\n",
    "orders_data_grouped = pd.merge(orders_data_grouped, total_orders_grouped, on=['start_region_hash','time_slot', 'weekday'])\n",
    "\n",
    "# calculate the supply variable as the difference between total_orders and order_gap\n",
    "orders_data_grouped['supply'] = orders_data_grouped['demand'] - orders_data_grouped['order_gap']\n",
    "\n",
    "print(orders_data_grouped.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      time_slot  weekday  temperature        pm25\n",
      "0             0        0     8.000000  134.200000\n",
      "1             0        1     7.500000  182.500000\n",
      "2             0        2     5.500000   73.500000\n",
      "3             0        3     5.500000  102.500000\n",
      "4             0        4     4.333333  171.333333\n",
      "...         ...      ...          ...         ...\n",
      "1003        143        2     5.500000  102.500000\n",
      "1004        143        3     2.666667   92.666667\n",
      "1005        143        4     5.500000  130.000000\n",
      "1006        143        5     8.000000  147.000000\n",
      "1007        143        6     8.000000  137.500000\n",
      "\n",
      "[1008 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# group the weather data by time slot\n",
    "# aggregate the mean of temperature and pm25\n",
    "weather_data_grouped = weather_data.groupby(['time_slot', 'weekday']).agg({'temperature': 'mean', 'pm25': 'mean'}).reset_index()\n",
    "\n",
    "print(weather_data_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      start_region_hash  time_slot  weekday  order_gap  \\\n",
      "0      08232402614a9b48895cc3d0aeb0e9f2          0        0          1   \n",
      "1      1afd7afbc81ecc1b13886a569d869e8a          0        0          3   \n",
      "2      1cbfbdd079ef93e74405c53fcfff8567          0        0          1   \n",
      "3      1ecbb52d73c522f184a6fc53128b1ea1          0        0          5   \n",
      "4      2301bc920194c95cf0c7486e5675243c          0        0          3   \n",
      "...                                 ...        ...      ...        ...   \n",
      "49661  bf44d327f0232325c6d5280926d7b37d         22        3          1   \n",
      "49662  ca064c2682ca48c6a21de012e87c0df5         22        3          3   \n",
      "49663  d4ec2125aff74eded207d2d915ef682f         22        3         30   \n",
      "49664  dd8d3b9665536d6e05b29c2648c0e69a         22        3          1   \n",
      "49665  fff4e8465d1e12621bc361276b6217cf         22        3          1   \n",
      "\n",
      "       demand  supply  temperature   pm25  \n",
      "0           1       0          8.0  134.2  \n",
      "1          97      94          8.0  134.2  \n",
      "2           8       7          8.0  134.2  \n",
      "3          17      12          8.0  134.2  \n",
      "4           7       4          8.0  134.2  \n",
      "...       ...     ...          ...    ...  \n",
      "49661       2       1          4.5  107.0  \n",
      "49662      15      12          4.5  107.0  \n",
      "49663     122      92          4.5  107.0  \n",
      "49664       1       0          4.5  107.0  \n",
      "49665       1       0          4.5  107.0  \n",
      "\n",
      "[49666 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# merge the orders data and weather data\n",
    "orders_weather_data = pd.merge(orders_data_grouped, weather_data_grouped, on=['time_slot', 'weekday'], how='inner' )\n",
    "\n",
    "print(orders_weather_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         region_hash  region_id  poi_ids\n",
      "0   90c5a34f06ac86aee0fd70e2adce7d8a          1   653376\n",
      "1   f2c8c4bb99e6377d21de71275afd6cd2          2   343537\n",
      "2   58c7a4888306d8ff3a641d1c0feccbe3          3    31125\n",
      "3   b26a240205c852804ff8758628c0a86a          4   187829\n",
      "4   4b9e4cf2fbdc8281b8a1f9f12b80ce4d          5    27888\n",
      "..                               ...        ...      ...\n",
      "56  a735449c5c09df639c35a7d61fad3ee5         62     2988\n",
      "57  0a5fef95db34383403d11cb6af937309         63    73704\n",
      "58  bf44d327f0232325c6d5280926d7b37d         64   211982\n",
      "59  825a21aa308dea206adb49c4b77c7805         65    91217\n",
      "60  1ecbb52d73c522f184a6fc53128b1ea1         66   138942\n",
      "\n",
      "[61 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# merge the poi_list class characteristics with the cluster_map\n",
    "# cluster_map: region_hash, region_id\n",
    "# poi_data: district_hash, poi_ids\n",
    "# merge on district_hash\n",
    "cluster_map_poi = pd.merge(cluster_map, poi_data, left_on='region_hash', right_on='region_hash', how='inner')\n",
    "\n",
    "# remove the region_hash column\n",
    "# cluster_map_poi = cluster_map_poi.drop(['region_id'], axis=1)\n",
    "\n",
    "print(cluster_map_poi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       time_slot  weekday  order_gap  demand  supply  temperature        pm25  \\\n",
      "0              0        0          1       1       0     8.000000  134.200000   \n",
      "1              0        3          1       2       1     5.500000  102.500000   \n",
      "2              0        4          5       5       0     4.333333  171.333333   \n",
      "3              0        5          2       2       0     5.600000  129.600000   \n",
      "4              0        6          3       5       2     8.000000  147.000000   \n",
      "...          ...      ...        ...     ...     ...          ...         ...   \n",
      "44624        134        0          3       5       2     9.000000  197.000000   \n",
      "44625        135        1          1       1       0     5.250000   76.500000   \n",
      "44626        135        3          1       2       1     5.250000  128.750000   \n",
      "44627        140        5          1       1       0     8.166667  145.666667   \n",
      "44628        143        0          1       1       0     7.500000  182.500000   \n",
      "\n",
      "       region_id  poi_ids  \n",
      "0             50    57270  \n",
      "1             50    57270  \n",
      "2             50    57270  \n",
      "3             50    57270  \n",
      "4             50    57270  \n",
      "...          ...      ...  \n",
      "44624         62     2988  \n",
      "44625         62     2988  \n",
      "44626         62     2988  \n",
      "44627         62     2988  \n",
      "44628         62     2988  \n",
      "\n",
      "[44629 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# merge the orders_data with the cluster_map_poi\n",
    "# orders_weather_data: start_district_hash, time_slot, weekday, order_gap, temperature, pm25\n",
    "# cluster_map_poi: region_id, poi_ids\n",
    "# merge on start_district_hash\n",
    "# print(orders_weather_data.head())\n",
    "# print(cluster_map_poi.head())\n",
    "\n",
    "orders_weather_cluster_map_poi = pd.merge(orders_weather_data, cluster_map_poi, left_on='start_region_hash', right_on='region_hash', how='inner')\n",
    "\n",
    "# remove the start_district_hash column\n",
    "orders_weather_cluster_map_poi = orders_weather_cluster_map_poi.drop(['start_region_hash', 'region_hash'], axis=1)\n",
    "\n",
    "print(orders_weather_cluster_map_poi)\n",
    "\n",
    "\n",
    "# save the data\n",
    "orders_weather_cluster_map_poi.to_csv('../dataset/processedData/orders_weather_cluster_map_poi.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
