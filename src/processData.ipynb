{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to clean and pre-process the data\n",
    "# import data using pandas\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_map finished\n",
      "cluster_map.head(): \n",
      "                         region_hash  region_id\n",
      "0  90c5a34f06ac86aee0fd70e2adce7d8a          1\n",
      "1  f2c8c4bb99e6377d21de71275afd6cd2          2\n",
      "2  58c7a4888306d8ff3a641d1c0feccbe3          3\n",
      "3  b26a240205c852804ff8758628c0a86a          4\n",
      "4  4b9e4cf2fbdc8281b8a1f9f12b80ce4d          5\n"
     ]
    }
   ],
   "source": [
    "# label the cluster map\n",
    "# labels:\n",
    "# region_hash, region_id\n",
    "\n",
    "columns = ['region_hash', 'region_id']\n",
    "# read the cluster map\n",
    "cluster_map = pd.read_csv('../dataset/training_data/cluster_map/cluster_map', sep='\\t', on_bad_lines='skip', header=None, names=columns)\n",
    "print('cluster_map finished')\n",
    "\n",
    "print('cluster_map.head(): \\n', cluster_map.head())\n",
    "\n",
    "\n",
    "# cluster_map.to_csv('../dataset/labeledData/cluster_map.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-01\n",
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-02\n",
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m glob\u001b[39m.\u001b[39mglob(\u001b[39m'\u001b[39m\u001b[39m../dataset/training_data/order_data/order_data_*\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     10\u001b[0m     \u001b[39m# file name\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mfilename: \u001b[39m\u001b[39m'\u001b[39m, f)\n\u001b[1;32m---> 12\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(f, sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m'\u001b[39;49m, on_bad_lines\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mskip\u001b[39;49m\u001b[39m'\u001b[39;49m, header\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, names\u001b[39m=\u001b[39;49mcolumns)\n\u001b[0;32m     13\u001b[0m     orders_data\u001b[39m.\u001b[39mappend(df)\n\u001b[0;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39morders_data finished\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     (\n\u001b[0;32m   1775\u001b[0m         index,\n\u001b[0;32m   1776\u001b[0m         columns,\n\u001b[0;32m   1777\u001b[0m         col_dict,\n\u001b[1;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1779\u001b[0m         nrows\n\u001b[0;32m   1780\u001b[0m     )\n\u001b[0;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 230\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    231\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1037\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1083\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1158\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\common.py:1433\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[39m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[0;32m   1425\u001b[0m     \u001b[39m#  here too.\u001b[39;00m\n\u001b[0;32m   1426\u001b[0m     \u001b[39m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[0;32m   1427\u001b[0m     \u001b[39m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[0;32m   1428\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m   1429\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[0;32m   1430\u001b[0m     )\n\u001b[1;32m-> 1433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m   1434\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1435\u001b[0m \u001b[39m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[0;32m   1436\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1476\u001b[0m \u001b[39m    False\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1478\u001b[0m     dtype \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(arr_or_dtype, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# label the data\n",
    "# label the orders data\n",
    "# labels:\n",
    "# order_id, driver_id, passenger_id, start_district_hash, dest_district_hash, price, time\n",
    "columns = ['order_id', 'driver_id', 'passenger_id', 'start_region_hash', 'dest_region_hash', 'price', 'time']\n",
    "\n",
    "# read the orders data\n",
    "orders_data = []\n",
    "for f in glob.glob('../dataset/training_data/order_data/order_data_*'):\n",
    "    # file name\n",
    "    print('filename: ', f)\n",
    "    df = pd.read_csv(f, sep='\\t', on_bad_lines='skip', header=None, names=columns)\n",
    "    orders_data.append(df)\n",
    "\n",
    "print('orders_data finished')\n",
    "orders_data = pd.concat(orders_data,  ignore_index=True)\n",
    "\n",
    "# print('orders_data.head(): ', orders_data.head())\n",
    "orders_data.to_csv('../dataset/labeledData/orders_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-01\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-02\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-03\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-04\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-05\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-06\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-07\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-08\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-09\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-10\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-11\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-12\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-13\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-14\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-15\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-16\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-17\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-18\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-19\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-20\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-21\n",
      "weather_data finished\n"
     ]
    }
   ],
   "source": [
    "# label the weather data\n",
    "# labels:\n",
    "# time, weather, temperature, pm25\n",
    "columns = ['time', 'weather', 'temperature', 'pm25']\n",
    "\n",
    "# print('weather_data.head(): \\n', weather_data.head())\n",
    "\n",
    "\n",
    "# # read the weather data\n",
    "weather_data = []\n",
    "for f in glob.glob('../dataset/training_data/weather_data/weather_data_*'):\n",
    "    # file name\n",
    "    print('filename: ', f)\n",
    "    df = pd.read_csv(f, sep='\\t', on_bad_lines='skip', header=None, names=columns)\n",
    "    weather_data.append(df)\n",
    "\n",
    "print('weather_data finished')\n",
    "weather_data = pd.concat(weather_data, ignore_index=True)\n",
    "\n",
    "\n",
    "# weather_data.to_csv('../dataset/labeledData/weather_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        region_hash  \\\n",
      "0  74c1c25f4b283fa74a5514307b0d0278   \n",
      "1  08f5b445ec6b29deba62e6fd8b0325a6   \n",
      "2  4b7f6f4e2bf237b6cc58f57142bea5c0   \n",
      "3  a814069db8d32f0fa6e188f41059c6e1   \n",
      "4  8316146a6f78cc6d9f113f0390859417   \n",
      "\n",
      "                                             poi_ids  \n",
      "0  [1#11:2241, 1#10:249, 24:1245, 25:3652, 20:334...  \n",
      "1  [20#7:249, 20#5:83, 2#7:166, 20#2:747, 20#1:99...  \n",
      "2  [4#16:249, 24:913, 25:332, 20:4316, 22:415, 4:...  \n",
      "3  [1#11:498, 24:332, 25:581, 20:5810, 22:2407, 4...  \n",
      "4  [20#7:581, 20#5:83, 20#4:415, 20#2:166, 20#1:6...  \n"
     ]
    }
   ],
   "source": [
    "# label the poi data\n",
    "# labels:\n",
    "# region_hash, poi_id \n",
    "# 1st column: district_hash\n",
    "# whole next column is: poi_id\n",
    "columns = ['region_hash', 'poi_id']\n",
    "\n",
    "\n",
    "\n",
    "# read the poi data\n",
    "poi_data = pd.read_csv('../dataset/training_data/poi_data/poi_data', sep='\\t', header=None, on_bad_lines='skip')\n",
    "\n",
    "# extract the district_hash column and the POI ID columns\n",
    "district_hash = poi_data.iloc[:, 0]\n",
    "poi_ids = poi_data.iloc[:, 1:]\n",
    "\n",
    "# combine all the POI IDs for each row into a list\n",
    "poi_ids_list = poi_ids.apply(lambda x: x.tolist(), axis=1)\n",
    "\n",
    "# combine the district_hash and poi_ids_list into a new DataFrame\n",
    "labeled_poi_data = pd.concat([district_hash, poi_ids_list], axis=1)\n",
    "labeled_poi_data.columns = ['region_hash', 'poi_ids']\n",
    "\n",
    "# print the result\n",
    "print(labeled_poi_data.head())\n",
    "\n",
    "# convert the column of lists to a list of lists\n",
    "# list_of_lists = df['list_col'].tolist()\n",
    "\n",
    "labeled_poi_data.to_csv('../dataset/labeledData/poi_data.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from the labeled data\n",
    "cluster_map = pd.read_csv('../dataset/labeledData/cluster_map.csv')\n",
    "orders_data = pd.read_csv('../dataset/labeledData/orders_data.csv')\n",
    "weather_data = pd.read_csv('../dataset/labeledData/weather_data.csv')\n",
    "poi_data = pd.read_csv('../dataset/labeledData/poi_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map time to time slot\n",
    "# devide day in 10 min time slots (144 time slots)\n",
    "\n",
    "\n",
    "# convert time to datetime\n",
    "orders_data['time'] = pd.to_datetime(orders_data['time'])\n",
    "weather_data['time'] = pd.to_datetime(weather_data['time'])\n",
    "\n",
    "# map time to time slot\n",
    "orders_data['time_slot'] = orders_data['time'].dt.hour * 6 + orders_data['time'].dt.minute // 10\n",
    "weather_data['time_slot'] = weather_data['time'].dt.hour * 6 + weather_data['time'].dt.minute // 10\n",
    "# remove the time column\n",
    "orders_data = orders_data.drop(['time'], axis=1)\n",
    "weather_data = weather_data.drop(['time'], axis=1)\n",
    "\n",
    "# print(orders_data.head())\n",
    "# print(weather_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     time_slot  order_gap\n",
      "0            0       5092\n",
      "1            1       5678\n",
      "2            2       6995\n",
      "3            3       8847\n",
      "4            4       9093\n",
      "..         ...        ...\n",
      "139        139       3463\n",
      "140        140       3200\n",
      "141        141       3620\n",
      "142        142       4084\n",
      "143        143       4039\n",
      "\n",
      "[144 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# group the orders data by time slot \n",
    "# aggregate count the number of orders where driver_id = NULL\n",
    "# this is supply demand deficit\n",
    "\n",
    "orders_data_grouped = orders_data[orders_data['driver_id'].isnull()].groupby(['time_slot', '']).agg({'order_id': 'count'}).rename(columns={'order_id': 'order_gap'}).reset_index()\n",
    "\n",
    "print(orders_data_grouped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     time_slot  temperature        pm25\n",
      "0            0     6.548387  133.838710\n",
      "1            1     6.176471  132.735294\n",
      "2            2     6.117647  131.911765\n",
      "3            3     6.088235  132.500000\n",
      "4            4     5.972222  132.305556\n",
      "..         ...          ...         ...\n",
      "139        139     6.387097  132.967742\n",
      "140        140     6.108108  129.540541\n",
      "141        141     5.970588  127.941176\n",
      "142        142     6.085714  125.971429\n",
      "143        143     6.125000  125.812500\n",
      "\n",
      "[144 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# group the weather data by time slot\n",
    "# aggregate the mean of temperature and pm25\n",
    "weather_data_grouped = weather_data.groupby(['time_slot']).agg({'temperature': 'mean', 'pm25': 'mean'}).reset_index()\n",
    "\n",
    "print(weather_data_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     time_slot  order_gap  temperature        pm25\n",
      "0            0       5092     6.548387  133.838710\n",
      "1            1       5678     6.176471  132.735294\n",
      "2            2       6995     6.117647  131.911765\n",
      "3            3       8847     6.088235  132.500000\n",
      "4            4       9093     5.972222  132.305556\n",
      "..         ...        ...          ...         ...\n",
      "139        139       3463     6.387097  132.967742\n",
      "140        140       3200     6.108108  129.540541\n",
      "141        141       3620     5.970588  127.941176\n",
      "142        142       4084     6.085714  125.971429\n",
      "143        143       4039     6.125000  125.812500\n",
      "\n",
      "[144 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# merge the orders data and weather data\n",
    "orders_weather_data = pd.merge(orders_data_grouped, weather_data_grouped, on='time_slot', how='inner')\n",
    "\n",
    "print(orders_weather_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   region_id                                            poi_ids\n",
      "0          1  ['4#16:415', '1#10:498', '24:2324', '25:1245',...\n",
      "1          2  ['4#16:166', '24:83', '25:581', '20:24153', '2...\n",
      "2          3  ['20#7:830', '20#4:166', '20#2:166', '20#1:157...\n",
      "3          4  ['4#16:249', '1#10:83', '24:581', '25:747', '2...\n",
      "4          5  ['20#7:581', '20#5:166', '20#4:83', '8#4:332',...\n"
     ]
    }
   ],
   "source": [
    "# merge the poi_list class characteristics with the cluster_map\n",
    "# cluster_map: region_hash, region_id\n",
    "# poi_data: district_hash, poi_ids\n",
    "# merge on district_hash\n",
    "cluster_map_poi = pd.merge(cluster_map, poi_data, left_on='region_hash', right_on='region_hash', how='inner')\n",
    "\n",
    "# remove the region_hash column\n",
    "cluster_map_poi = cluster_map_poi.drop(['region_hash'], axis=1)\n",
    "\n",
    "print(cluster_map_poi.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the orders_data with the cluster_map_poi\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
