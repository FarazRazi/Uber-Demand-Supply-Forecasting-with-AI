{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to clean and pre-process the data\n",
    "# import data using pandas\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label the cluster map\n",
    "# labels:\n",
    "# region_hash, region_id\n",
    "\n",
    "columns = ['region_hash', 'region_id']\n",
    "# read the cluster map\n",
    "cluster_map = pd.read_csv('../dataset/training_data/cluster_map/cluster_map', sep='\\t', on_bad_lines='skip', header=None, names=columns)\n",
    "print('cluster_map finished')\n",
    "\n",
    "print('cluster_map.head(): \\n', cluster_map.head())\n",
    "\n",
    "\n",
    "cluster_map.to_csv('../dataset/labeledData/cluster_map.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label the orders data\n",
    "# labels:\n",
    "# order_id, driver_id, passenger_id, start_district_hash, dest_district_hash, price, time\n",
    "columns = ['order_id', 'driver_id', 'passenger_id', 'start_region_hash', 'dest_region_hash', 'price', 'time']\n",
    "\n",
    "# read the orders data\n",
    "orders_data = []\n",
    "for f in glob.glob('../dataset/training_data/order_data/order_data_*'):\n",
    "    # file name\n",
    "    print('filename: ', f)\n",
    "    df = pd.read_csv(f, sep='\\t', on_bad_lines='skip', header=None, names=columns)\n",
    "    orders_data.append(df)\n",
    "\n",
    "print('orders_data finished')\n",
    "orders_data = pd.concat(orders_data,  ignore_index=True)\n",
    "\n",
    "# print('orders_data.head(): ', orders_data.head())\n",
    "orders_data.to_csv('../dataset/labeledData/orders_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label the weather data\n",
    "# labels:\n",
    "# time, weather, temperature, pm25\n",
    "columns = ['time', 'weather', 'temperature', 'pm25']\n",
    "\n",
    "# print('weather_data.head(): \\n', weather_data.head())\n",
    "\n",
    "\n",
    "# # read the weather data\n",
    "weather_data = []\n",
    "for f in glob.glob('../dataset/training_data/weather_data/weather_data_*'):\n",
    "    # file name\n",
    "    print('filename: ', f)\n",
    "    df = pd.read_csv(f, sep='\\t', on_bad_lines='skip', header=None, names=columns)\n",
    "    weather_data.append(df)\n",
    "\n",
    "print('weather_data finished')\n",
    "weather_data = pd.concat(weather_data, ignore_index=True)\n",
    "\n",
    "\n",
    "# weather_data.to_csv('../dataset/labeledData/weather_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label the poi data\n",
    "# labels:\n",
    "# region_hash, poi_id \n",
    "# 1st column: district_hash\n",
    "# whole next column is: poi_id\n",
    "columns = ['region_hash', 'poi_id']\n",
    "\n",
    "\n",
    "\n",
    "# read the poi data\n",
    "poi_data = pd.read_csv('../dataset/training_data/poi_data/poi_data', sep='\\t', header=None, on_bad_lines='skip')\n",
    "\n",
    "# extract the district_hash column and the POI ID columns\n",
    "district_hash = poi_data.iloc[:, 0]\n",
    "poi_ids = poi_data.iloc[:, 1:]\n",
    "\n",
    "# combine all the POI IDs for each row into a list\n",
    "poi_ids_list = poi_ids.apply(lambda x: x.tolist(), axis=1)\n",
    "\n",
    "# combine the district_hash and poi_ids_list into a new DataFrame\n",
    "labeled_poi_data = pd.concat([district_hash, poi_ids_list], axis=1)\n",
    "labeled_poi_data.columns = ['region_hash', 'poi_ids']\n",
    "\n",
    "# print the result\n",
    "# print(labeled_poi_data.head())\n",
    "\n",
    "# updated list\n",
    "updated_list = []\n",
    "\n",
    "# convert the column of lists to a list of lists\n",
    "list_of_lists_poi_id = labeled_poi_data['poi_ids'].tolist()\n",
    "\n",
    "# poi format poi_id = class1#class2:numofFacilities\n",
    "# seperate numofFacilities from list_of_lists_poi_id and sum them up\n",
    "\n",
    "# for each list in list_of_lists_poi_id \n",
    "# change the list of poi_id to weighted sum of numofFacilities \n",
    "for poi_list in list_of_lists_poi_id:\n",
    "    weighted_sum = 0\n",
    "    for poi in poi_list:\n",
    "        if(pd.isna(poi)==False):\n",
    "            poi_id, num_of_facilities = poi.split(':')\n",
    "            poi_class = poi_id.split('#')\n",
    "            # combine the class1 and class2 numbers\n",
    "            if(len(poi_class) == 1):\n",
    "                poi_class[0] = '0' + poi_class[0]\n",
    "            else:\n",
    "                poi_number = poi_class[0] + '' + poi_class[1]\n",
    "            weighted_sum += int(num_of_facilities) * int(poi_number)\n",
    "\n",
    "    updated_list.append(weighted_sum)\n",
    "\n",
    "# print(list_of_lists_poi_id)\n",
    "\n",
    "# change labeled_poi_data['poi_ids'] to list_of_lists_poi_id\n",
    "labeled_poi_data['poi_ids'] = updated_list\n",
    "\n",
    "print('labeled_poi_data.head(): ', labeled_poi_data.head())\n",
    "\n",
    "labeled_poi_data.to_csv('../dataset/labeledData/poi_data.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from the labeled data\n",
    "cluster_map = pd.read_csv('../dataset/labeledData/cluster_map.csv')\n",
    "orders_data = pd.read_csv('../dataset/labeledData/orders_data.csv')\n",
    "weather_data = pd.read_csv('../dataset/labeledData/weather_data.csv')\n",
    "poi_data = pd.read_csv('../dataset/labeledData/poi_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           order_id                         driver_id  \\\n",
      "0  97ebd0c6680f7c0535dbfdead6e51b4b  dd65fa250fca2833a3a8c16d2cf0457c   \n",
      "1  92c3ac9251cc9b5aab90b114a1e363be  c077e0297639edcb1df6189e8cda2c3d   \n",
      "2  abeefc3e2aec952468e2fd42a1649640  86dbc1b68de435957c61b5a523854b69   \n",
      "3  cb31d0be64cda3cc66b46617bf49a05c  4fadfa6eeaa694742de036dddf02b0c4   \n",
      "4  139d492189ae5a933122c098f63252b3                               NaN   \n",
      "\n",
      "                       passenger_id                 start_region_hash  \\\n",
      "0  ed180d7daf639d936f1aeae4f7fb482f  4725c39a5e5f4c188d382da3910b3f3f   \n",
      "1  191a180f0a262aff3267775c4fac8972  82cc4851f9e4faa4e54309f8bb73fd7c   \n",
      "2  7029e813bb3de8cc73a8615e2785070c  fff4e8465d1e12621bc361276b6217cf   \n",
      "3  21dc133ac68e4c07803d1c2f48988a83  4b7f6f4e2bf237b6cc58f57142bea5c0   \n",
      "4  26963cc76da2d8450d8f23fc357db987  fc34648599753c9e74ab238e9a4a07ad   \n",
      "\n",
      "                   dest_region_hash  price  time_slot  weekday  \n",
      "0  3e12208dd0be281c92a6ab57d9a6fb32   24.0         81        4  \n",
      "1  b05379ac3f9b7d99370d443cfd5dcc28    2.0         58        4  \n",
      "2  fff4e8465d1e12621bc361276b6217cf    9.0        110        4  \n",
      "3  4b7f6f4e2bf237b6cc58f57142bea5c0   11.0        133        4  \n",
      "4  87285a66236346350541b8815c5fae94    4.0        102        4  \n",
      "   weather  temperature  pm25  time_slot  weekday\n",
      "0        1          4.0   177          0        4\n",
      "1        1          3.0   177          0        4\n",
      "2        1          3.0   177          1        4\n",
      "3        1          3.0   177          1        4\n",
      "4        1          3.0   177          2        4\n"
     ]
    }
   ],
   "source": [
    "# map time to time slot\n",
    "# devide day in 10 min time slots (144 time slots)\n",
    "\n",
    "\n",
    "# convert time to datetime\n",
    "orders_data['time'] = pd.to_datetime(orders_data['time'])\n",
    "weather_data['time'] = pd.to_datetime(weather_data['time'])\n",
    "\n",
    "# map time to time slot\n",
    "orders_data['time_slot'] = orders_data['time'].dt.hour * 6 + orders_data['time'].dt.minute // 10\n",
    "weather_data['time_slot'] = weather_data['time'].dt.hour * 6 + weather_data['time'].dt.minute // 10\n",
    "\n",
    "# map time to time slot with weekday\n",
    "orders_data['weekday'] = orders_data['time'].dt.weekday\n",
    "weather_data['weekday'] = weather_data['time'].dt.weekday\n",
    "\n",
    "# remove the time column\n",
    "orders_data = orders_data.drop(['time'], axis=1)\n",
    "weather_data = weather_data.drop(['time'], axis=1)\n",
    "\n",
    "print(orders_data.head())\n",
    "print(weather_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  start_region_hash                  dest_region_hash  \\\n",
      "0  08232402614a9b48895cc3d0aeb0e9f2  08232402614a9b48895cc3d0aeb0e9f2   \n",
      "1  08232402614a9b48895cc3d0aeb0e9f2  08232402614a9b48895cc3d0aeb0e9f2   \n",
      "2  08232402614a9b48895cc3d0aeb0e9f2  08232402614a9b48895cc3d0aeb0e9f2   \n",
      "3  08232402614a9b48895cc3d0aeb0e9f2  08232402614a9b48895cc3d0aeb0e9f2   \n",
      "4  08232402614a9b48895cc3d0aeb0e9f2  08232402614a9b48895cc3d0aeb0e9f2   \n",
      "\n",
      "   time_slot  weekday  order_gap  demand      price  supply  \n",
      "0          0        4          5       5   9.000000       0  \n",
      "1          1        4          3       3  10.666667       0  \n",
      "2          1        6          1       1  13.000000       0  \n",
      "3          2        4          3       3  45.333333       0  \n",
      "4          3        6          1       1  11.000000       0  \n"
     ]
    }
   ],
   "source": [
    "# group the orders data by time slot \n",
    "# aggregate count the number of orders where driver_id = NULL\n",
    "\n",
    "# this is supply demand deficit - order gap\n",
    "orders_data_grouped = orders_data[orders_data['driver_id'].isnull()].groupby(['start_region_hash','dest_region_hash','time_slot', 'weekday']).agg({'order_id': 'count'}).rename(columns={'order_id': 'order_gap'}).reset_index()\n",
    "\n",
    "# this is the total demand\n",
    "total_orders_grouped = orders_data.groupby(['start_region_hash','dest_region_hash','time_slot', 'weekday']).agg({'order_id': 'count', 'price': 'mean'}).rename(columns={'order_id': 'demand'}).reset_index()\n",
    "\n",
    "# merge the two dataframes on the region, time slot and weekday\n",
    "orders_data_grouped = pd.merge(orders_data_grouped, total_orders_grouped, on=['start_region_hash','dest_region_hash','time_slot', 'weekday'])\n",
    "\n",
    "# calculate the supply variable as the difference between total_orders and order_gap\n",
    "orders_data_grouped['supply'] = orders_data_grouped['demand'] - orders_data_grouped['order_gap']\n",
    "\n",
    "print(orders_data_grouped.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      time_slot  weekday  temperature        pm25\n",
      "0             0        0     8.000000  134.200000\n",
      "1             0        1     7.500000  182.500000\n",
      "2             0        2     5.500000   73.500000\n",
      "3             0        3     5.500000  102.500000\n",
      "4             0        4     4.333333  171.333333\n",
      "...         ...      ...          ...         ...\n",
      "1003        143        2     5.500000  102.500000\n",
      "1004        143        3     2.666667   92.666667\n",
      "1005        143        4     5.500000  130.000000\n",
      "1006        143        5     8.000000  147.000000\n",
      "1007        143        6     8.000000  137.500000\n",
      "\n",
      "[1008 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# group the weather data by time slot\n",
    "# aggregate the mean of temperature and pm25\n",
    "weather_data_grouped = weather_data.groupby(['time_slot', 'weekday']).agg({'temperature': 'mean', 'pm25': 'mean'}).reset_index()\n",
    "\n",
    "print(weather_data_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       start_region_hash                  dest_region_hash  \\\n",
      "0       08232402614a9b48895cc3d0aeb0e9f2  08232402614a9b48895cc3d0aeb0e9f2   \n",
      "1       0a5fef95db34383403d11cb6af937309  c119d09aebdac22f875d38fd982bd24b   \n",
      "2       1afd7afbc81ecc1b13886a569d869e8a  0c48862b9748fe682b4c8ee996ebe26a   \n",
      "3       1afd7afbc81ecc1b13886a569d869e8a  1afd7afbc81ecc1b13886a569d869e8a   \n",
      "4       1afd7afbc81ecc1b13886a569d869e8a  2407d482f0ffa22a947068f2551fe62c   \n",
      "...                                  ...                               ...   \n",
      "262915  d4ec2125aff74eded207d2d915ef682f  b05379ac3f9b7d99370d443cfd5dcc28   \n",
      "262916  d4ec2125aff74eded207d2d915ef682f  d4ec2125aff74eded207d2d915ef682f   \n",
      "262917  d4ec2125aff74eded207d2d915ef682f  ed8eb1876d270f25e29fe4339ad41524   \n",
      "262918  dd8d3b9665536d6e05b29c2648c0e69a  dd8d3b9665536d6e05b29c2648c0e69a   \n",
      "262919  fff4e8465d1e12621bc361276b6217cf  8316146a6f78cc6d9f113f0390859417   \n",
      "\n",
      "        time_slot  weekday  order_gap  demand       price  supply  \\\n",
      "0               0        4          5       5    9.000000       0   \n",
      "1               0        4          3       4   20.000000       1   \n",
      "2               0        4          1       1  418.000000       0   \n",
      "3               0        4          5      68   10.441176      63   \n",
      "4               0        4          2      30   20.033333      28   \n",
      "...           ...      ...        ...     ...         ...     ...   \n",
      "262915         22        3          1       3   29.666667       2   \n",
      "262916         22        3         12      34    8.441176      22   \n",
      "262917         22        3          2       2   45.500000       0   \n",
      "262918         22        3          1       1    4.000000       0   \n",
      "262919         22        3          1       1   11.000000       0   \n",
      "\n",
      "        temperature        pm25  \n",
      "0          4.333333  171.333333  \n",
      "1          4.333333  171.333333  \n",
      "2          4.333333  171.333333  \n",
      "3          4.333333  171.333333  \n",
      "4          4.333333  171.333333  \n",
      "...             ...         ...  \n",
      "262915     4.500000  107.000000  \n",
      "262916     4.500000  107.000000  \n",
      "262917     4.500000  107.000000  \n",
      "262918     4.500000  107.000000  \n",
      "262919     4.500000  107.000000  \n",
      "\n",
      "[262920 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# merge the orders data and weather data\n",
    "orders_weather_data = pd.merge(orders_data_grouped, weather_data_grouped, on=['time_slot', 'weekday'], how='inner' )\n",
    "\n",
    "print(orders_weather_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         region_hash  region_id    poi_ids\n",
      "0   90c5a34f06ac86aee0fd70e2adce7d8a          1  118257404\n",
      "1   f2c8c4bb99e6377d21de71275afd6cd2          2   68155035\n",
      "2   58c7a4888306d8ff3a641d1c0feccbe3          3    5013449\n",
      "3   b26a240205c852804ff8758628c0a86a          4   42874231\n",
      "4   4b9e4cf2fbdc8281b8a1f9f12b80ce4d          5    4327122\n",
      "..                               ...        ...        ...\n",
      "56  a735449c5c09df639c35a7d61fad3ee5         62     363374\n",
      "57  0a5fef95db34383403d11cb6af937309         63   12680076\n",
      "58  bf44d327f0232325c6d5280926d7b37d         64   41160364\n",
      "59  825a21aa308dea206adb49c4b77c7805         65   16275636\n",
      "60  1ecbb52d73c522f184a6fc53128b1ea1         66   23374709\n",
      "\n",
      "[61 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# merge the poi_list class characteristics with the cluster_map\n",
    "# cluster_map: region_hash, region_id\n",
    "# poi_data: district_hash, poi_ids\n",
    "# merge on district_hash\n",
    "cluster_map_poi = pd.merge(cluster_map, poi_data, left_on='region_hash', right_on='region_hash', how='inner')\n",
    "\n",
    "# remove the region_hash column\n",
    "# cluster_map_poi = cluster_map_poi.drop(['region_id'], axis=1)\n",
    "\n",
    "print(cluster_map_poi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        time_slot  weekday  order_gap  demand      price  supply  temperature  \\\n",
      "0               0        4          5       5   9.000000       0     4.333333   \n",
      "1               1        4          3       3  10.666667       0     4.500000   \n",
      "2               1        6          1       1  13.000000       0     8.000000   \n",
      "3               2        4          3       3  45.333333       0     4.000000   \n",
      "4               3        6          1       1  11.000000       0     8.000000   \n",
      "...           ...      ...        ...     ...        ...     ...          ...   \n",
      "100164        109        5          1       1   1.000000       0    10.600000   \n",
      "100165        126        1          2       3   2.000000       1     5.500000   \n",
      "100166        135        1          1       1   1.000000       0     5.250000   \n",
      "100167        133        0          1       1   4.000000       0    12.000000   \n",
      "100168         78        4          2       3  47.000000       1     8.750000   \n",
      "\n",
      "              pm25  start_region_id  start_poi_ids  dest_region_id  \\\n",
      "0       171.333333               50        8576971              50   \n",
      "1       168.500000               50        8576971              50   \n",
      "2       147.000000               50        8576971              50   \n",
      "3       171.333333               50        8576971              50   \n",
      "4       147.000000               50        8576971              50   \n",
      "...            ...              ...            ...             ...   \n",
      "100164  114.400000               62         363374              62   \n",
      "100165   67.000000               62         363374              62   \n",
      "100166   76.500000               62         363374              62   \n",
      "100167  194.000000               62         363374              62   \n",
      "100168  172.000000               45        4484573              62   \n",
      "\n",
      "        dest_poi_ids  \n",
      "0            8576971  \n",
      "1            8576971  \n",
      "2            8576971  \n",
      "3            8576971  \n",
      "4            8576971  \n",
      "...              ...  \n",
      "100164        363374  \n",
      "100165        363374  \n",
      "100166        363374  \n",
      "100167        363374  \n",
      "100168        363374  \n",
      "\n",
      "[100169 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# merge the orders_data with the cluster_map_poi\n",
    "# orders_weather_data: start_district_hash, time_slot, weekday, order_gap, temperature, pm25\n",
    "# cluster_map_poi: region_id, poi_ids\n",
    "# merge on start_district_hash\n",
    "# print(orders_weather_data.head())\n",
    "# print(cluster_map_poi.head())\n",
    "\n",
    "orders_weather_cluster_map_poi = pd.merge(orders_weather_data, cluster_map_poi, left_on='start_region_hash', right_on='region_hash', how='inner')\n",
    "orders_weather_cluster_map_poi=orders_weather_cluster_map_poi.rename(columns={'poi_ids': 'start_poi_ids'}).rename(columns={'region_id': 'start_region_id'})\n",
    "\n",
    "orders_weather_cluster_map_poi = pd.merge(orders_weather_cluster_map_poi, cluster_map_poi, left_on='dest_region_hash', right_on='region_hash', how='inner')\n",
    "orders_weather_cluster_map_poi=orders_weather_cluster_map_poi.rename(columns={'poi_ids': 'dest_poi_ids'}).rename(columns={'region_id': 'dest_region_id'})\n",
    "\n",
    "# remove the start_district_hash column\n",
    "orders_weather_cluster_map_poi = orders_weather_cluster_map_poi.drop(['start_region_hash', 'dest_region_hash', 'region_hash_x','region_hash_y'], axis=1)\n",
    "\n",
    "print(orders_weather_cluster_map_poi)\n",
    "\n",
    "\n",
    "# save the data\n",
    "orders_weather_cluster_map_poi.to_csv('../dataset/processedData/orders_weather_cluster_map_poi.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
