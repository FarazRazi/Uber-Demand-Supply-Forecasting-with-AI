{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to clean and pre-process the data\n",
    "# import data using pandas\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_map finished\n",
      "cluster_map.head(): \n",
      "                         region_hash  region_id\n",
      "0  90c5a34f06ac86aee0fd70e2adce7d8a          1\n",
      "1  f2c8c4bb99e6377d21de71275afd6cd2          2\n",
      "2  58c7a4888306d8ff3a641d1c0feccbe3          3\n",
      "3  b26a240205c852804ff8758628c0a86a          4\n",
      "4  4b9e4cf2fbdc8281b8a1f9f12b80ce4d          5\n"
     ]
    }
   ],
   "source": [
    "# label the cluster map\n",
    "# labels:\n",
    "# region_hash, region_id\n",
    "\n",
    "columns = ['region_hash', 'region_id']\n",
    "# read the cluster map\n",
    "cluster_map = pd.read_csv('../dataset/training_data/cluster_map/cluster_map', sep='\\t', on_bad_lines='skip', header=None, names=columns)\n",
    "print('cluster_map finished')\n",
    "\n",
    "print('cluster_map.head(): \\n', cluster_map.head())\n",
    "\n",
    "\n",
    "# cluster_map.to_csv('../dataset/labeledData/cluster_map.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-01\n",
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-02\n",
      "filename:  ../dataset/training_data/order_data\\order_data_2016-01-03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m glob\u001b[39m.\u001b[39mglob(\u001b[39m'\u001b[39m\u001b[39m../dataset/training_data/order_data/order_data_*\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     10\u001b[0m     \u001b[39m# file name\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mfilename: \u001b[39m\u001b[39m'\u001b[39m, f)\n\u001b[1;32m---> 12\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(f, sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m'\u001b[39;49m, on_bad_lines\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mskip\u001b[39;49m\u001b[39m'\u001b[39;49m, header\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, names\u001b[39m=\u001b[39;49mcolumns)\n\u001b[0;32m     13\u001b[0m     orders_data\u001b[39m.\u001b[39mappend(df)\n\u001b[0;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39morders_data finished\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     (\n\u001b[0;32m   1775\u001b[0m         index,\n\u001b[0;32m   1776\u001b[0m         columns,\n\u001b[0;32m   1777\u001b[0m         col_dict,\n\u001b[1;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1779\u001b[0m         nrows\n\u001b[0;32m   1780\u001b[0m     )\n\u001b[0;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 230\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    231\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1037\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1083\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1158\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\common.py:1433\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[39m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[0;32m   1425\u001b[0m     \u001b[39m#  here too.\u001b[39;00m\n\u001b[0;32m   1426\u001b[0m     \u001b[39m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[0;32m   1427\u001b[0m     \u001b[39m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[0;32m   1428\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m   1429\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[0;32m   1430\u001b[0m     )\n\u001b[1;32m-> 1433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m   1434\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1435\u001b[0m \u001b[39m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[0;32m   1436\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1476\u001b[0m \u001b[39m    False\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1478\u001b[0m     dtype \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(arr_or_dtype, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# label the data\n",
    "# label the orders data\n",
    "# labels:\n",
    "# order_id, driver_id, passenger_id, start_district_hash, dest_district_hash, price, time\n",
    "columns = ['order_id', 'driver_id', 'passenger_id', 'start_region_hash', 'dest_region_hash', 'price', 'time']\n",
    "\n",
    "# read the orders data\n",
    "orders_data = []\n",
    "for f in glob.glob('../dataset/training_data/order_data/order_data_*'):\n",
    "    # file name\n",
    "    print('filename: ', f)\n",
    "    df = pd.read_csv(f, sep='\\t', on_bad_lines='skip', header=None, names=columns)\n",
    "    orders_data.append(df)\n",
    "\n",
    "print('orders_data finished')\n",
    "orders_data = pd.concat(orders_data,  ignore_index=True)\n",
    "\n",
    "# print('orders_data.head(): ', orders_data.head())\n",
    "orders_data.to_csv('../dataset/labeledData/orders_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-01\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-02\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-03\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-04\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-05\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-06\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-07\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-08\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-09\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-10\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-11\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-12\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-13\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-14\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-15\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-16\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-17\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-18\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-19\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-20\n",
      "filename:  ../dataset/training_data/weather_data\\weather_data_2016-01-21\n",
      "weather_data finished\n"
     ]
    }
   ],
   "source": [
    "# label the weather data\n",
    "# labels:\n",
    "# time, weather, temperature, pm25\n",
    "columns = ['time', 'weather', 'temperature', 'pm25']\n",
    "\n",
    "# print('weather_data.head(): \\n', weather_data.head())\n",
    "\n",
    "\n",
    "# # read the weather data\n",
    "weather_data = []\n",
    "for f in glob.glob('../dataset/training_data/weather_data/weather_data_*'):\n",
    "    # file name\n",
    "    print('filename: ', f)\n",
    "    df = pd.read_csv(f, sep='\\t', on_bad_lines='skip', header=None, names=columns)\n",
    "    weather_data.append(df)\n",
    "\n",
    "print('weather_data finished')\n",
    "weather_data = pd.concat(weather_data, ignore_index=True)\n",
    "\n",
    "\n",
    "# weather_data.to_csv('../dataset/labeledData/weather_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled_poi_data.head():                          region_hash  poi_ids\n",
      "0  74c1c25f4b283fa74a5514307b0d0278   878555\n",
      "1  08f5b445ec6b29deba62e6fd8b0325a6    51128\n",
      "2  4b7f6f4e2bf237b6cc58f57142bea5c0   104248\n",
      "3  a814069db8d32f0fa6e188f41059c6e1   121844\n",
      "4  8316146a6f78cc6d9f113f0390859417    37599\n"
     ]
    }
   ],
   "source": [
    "# label the poi data\n",
    "# labels:\n",
    "# region_hash, poi_id \n",
    "# 1st column: district_hash\n",
    "# whole next column is: poi_id\n",
    "columns = ['region_hash', 'poi_id']\n",
    "\n",
    "\n",
    "\n",
    "# read the poi data\n",
    "poi_data = pd.read_csv('../dataset/training_data/poi_data/poi_data', sep='\\t', header=None, on_bad_lines='skip')\n",
    "\n",
    "# extract the district_hash column and the POI ID columns\n",
    "district_hash = poi_data.iloc[:, 0]\n",
    "poi_ids = poi_data.iloc[:, 1:]\n",
    "\n",
    "# combine all the POI IDs for each row into a list\n",
    "poi_ids_list = poi_ids.apply(lambda x: x.tolist(), axis=1)\n",
    "\n",
    "# combine the district_hash and poi_ids_list into a new DataFrame\n",
    "labeled_poi_data = pd.concat([district_hash, poi_ids_list], axis=1)\n",
    "labeled_poi_data.columns = ['region_hash', 'poi_ids']\n",
    "\n",
    "# print the result\n",
    "# print(labeled_poi_data.head())\n",
    "\n",
    "# updated list\n",
    "updated_list = []\n",
    "\n",
    "# convert the column of lists to a list of lists\n",
    "list_of_lists_poi_id = labeled_poi_data['poi_ids'].tolist()\n",
    "\n",
    "# poi format poi_id = class:numofFacilities\n",
    "# seperate numofFacilities from list_of_lists_poi_id and sum them up\n",
    "\n",
    "# for each list in list_of_lists_poi_id \n",
    "# change the list of poi_id to sum of numofFacilities \n",
    "for list in list_of_lists_poi_id:\n",
    "    sum = 0\n",
    "    for poi in list:\n",
    "        if(pd.isna(poi)==False):\n",
    "            poi = poi.split(':')\n",
    "            poi[1] = int(poi[1])\n",
    "            sum += poi[1]\n",
    "    updated_list.append(sum)\n",
    "\n",
    "# print(list_of_lists_poi_id)\n",
    "\n",
    "# change labeled_poi_data['poi_ids'] to list_of_lists_poi_id\n",
    "labeled_poi_data['poi_ids'] = updated_list\n",
    "\n",
    "print('labeled_poi_data.head(): ', labeled_poi_data.head())\n",
    "\n",
    "labeled_poi_data.to_csv('../dataset/labeledData/poi_data.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from the labeled data\n",
    "cluster_map = pd.read_csv('../dataset/labeledData/cluster_map.csv')\n",
    "orders_data = pd.read_csv('../dataset/labeledData/orders_data.csv')\n",
    "weather_data = pd.read_csv('../dataset/labeledData/weather_data.csv')\n",
    "poi_data = pd.read_csv('../dataset/labeledData/poi_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           order_id                         driver_id  \\\n",
      "0  97ebd0c6680f7c0535dbfdead6e51b4b  dd65fa250fca2833a3a8c16d2cf0457c   \n",
      "1  92c3ac9251cc9b5aab90b114a1e363be  c077e0297639edcb1df6189e8cda2c3d   \n",
      "2  abeefc3e2aec952468e2fd42a1649640  86dbc1b68de435957c61b5a523854b69   \n",
      "3  cb31d0be64cda3cc66b46617bf49a05c  4fadfa6eeaa694742de036dddf02b0c4   \n",
      "4  139d492189ae5a933122c098f63252b3                               NaN   \n",
      "\n",
      "                       passenger_id               start_district_hash  \\\n",
      "0  ed180d7daf639d936f1aeae4f7fb482f  4725c39a5e5f4c188d382da3910b3f3f   \n",
      "1  191a180f0a262aff3267775c4fac8972  82cc4851f9e4faa4e54309f8bb73fd7c   \n",
      "2  7029e813bb3de8cc73a8615e2785070c  fff4e8465d1e12621bc361276b6217cf   \n",
      "3  21dc133ac68e4c07803d1c2f48988a83  4b7f6f4e2bf237b6cc58f57142bea5c0   \n",
      "4  26963cc76da2d8450d8f23fc357db987  fc34648599753c9e74ab238e9a4a07ad   \n",
      "\n",
      "                 dest_district_hash  price  time_slot  weekday  \n",
      "0  3e12208dd0be281c92a6ab57d9a6fb32   24.0         81        4  \n",
      "1  b05379ac3f9b7d99370d443cfd5dcc28    2.0         58        4  \n",
      "2  fff4e8465d1e12621bc361276b6217cf    9.0        110        4  \n",
      "3  4b7f6f4e2bf237b6cc58f57142bea5c0   11.0        133        4  \n",
      "4  87285a66236346350541b8815c5fae94    4.0        102        4  \n",
      "   weather  temperature  pm25  time_slot  weekday\n",
      "0        1          4.0   177          0        4\n",
      "1        1          3.0   177          0        4\n",
      "2        1          3.0   177          1        4\n",
      "3        1          3.0   177          1        4\n",
      "4        1          3.0   177          2        4\n"
     ]
    }
   ],
   "source": [
    "# map time to time slot\n",
    "# devide day in 10 min time slots (144 time slots)\n",
    "\n",
    "\n",
    "# convert time to datetime\n",
    "orders_data['time'] = pd.to_datetime(orders_data['time'])\n",
    "weather_data['time'] = pd.to_datetime(weather_data['time'])\n",
    "\n",
    "# map time to time slot with weekday\n",
    "orders_data['time_slot'] = orders_data['time'].dt.hour * 6 + orders_data['time'].dt.minute // 10\n",
    "weather_data['time_slot'] = weather_data['time'].dt.hour * 6 + weather_data['time'].dt.minute // 10\n",
    "\n",
    "# map time to time slot with weekday\n",
    "orders_data['weekday'] = orders_data['time'].dt.weekday\n",
    "weather_data['weekday'] = weather_data['time'].dt.weekday\n",
    "\n",
    "# remove the time column\n",
    "orders_data = orders_data.drop(['time'], axis=1)\n",
    "weather_data = weather_data.drop(['time'], axis=1)\n",
    "\n",
    "print(orders_data.head())\n",
    "print(weather_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    start_district_hash  time_slot  weekday  order_gap\n",
      "0      08232402614a9b48895cc3d0aeb0e9f2          0        0          1\n",
      "1      08232402614a9b48895cc3d0aeb0e9f2          0        3          1\n",
      "2      08232402614a9b48895cc3d0aeb0e9f2          0        4          5\n",
      "3      08232402614a9b48895cc3d0aeb0e9f2          0        5          2\n",
      "4      08232402614a9b48895cc3d0aeb0e9f2          0        6          3\n",
      "...                                 ...        ...      ...        ...\n",
      "49661  fff4e8465d1e12621bc361276b6217cf        142        6          1\n",
      "49662  fff4e8465d1e12621bc361276b6217cf        143        0          3\n",
      "49663  fff4e8465d1e12621bc361276b6217cf        143        1          2\n",
      "49664  fff4e8465d1e12621bc361276b6217cf        143        3          1\n",
      "49665  fff4e8465d1e12621bc361276b6217cf        143        4          1\n",
      "\n",
      "[49666 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# group the orders data by time slot \n",
    "# aggregate count the number of orders where driver_id = NULL\n",
    "# this is supply demand deficit\n",
    "\n",
    "orders_data_grouped = orders_data[orders_data['driver_id'].isnull()].groupby(['start_district_hash','time_slot', 'weekday', ]).agg({'order_id': 'count'}).rename(columns={'order_id': 'order_gap'}).reset_index()\n",
    "\n",
    "print(orders_data_grouped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     time_slot  temperature        pm25\n",
      "0            0     6.548387  133.838710\n",
      "1            1     6.176471  132.735294\n",
      "2            2     6.117647  131.911765\n",
      "3            3     6.088235  132.500000\n",
      "4            4     5.972222  132.305556\n",
      "..         ...          ...         ...\n",
      "139        139     6.387097  132.967742\n",
      "140        140     6.108108  129.540541\n",
      "141        141     5.970588  127.941176\n",
      "142        142     6.085714  125.971429\n",
      "143        143     6.125000  125.812500\n",
      "\n",
      "[144 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# group the weather data by time slot\n",
    "# aggregate the mean of temperature and pm25\n",
    "weather_data_grouped = weather_data.groupby(['time_slot']).agg({'temperature': 'mean', 'pm25': 'mean'}).reset_index()\n",
    "\n",
    "print(weather_data_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    start_district_hash  time_slot  weekday  order_gap  \\\n",
      "0      08232402614a9b48895cc3d0aeb0e9f2          0        0          1   \n",
      "1      08232402614a9b48895cc3d0aeb0e9f2          0        3          1   \n",
      "2      08232402614a9b48895cc3d0aeb0e9f2          0        4          5   \n",
      "3      08232402614a9b48895cc3d0aeb0e9f2          0        5          2   \n",
      "4      08232402614a9b48895cc3d0aeb0e9f2          0        6          3   \n",
      "...                                 ...        ...      ...        ...   \n",
      "49661  fc34648599753c9e74ab238e9a4a07ad         25        5          9   \n",
      "49662  fc34648599753c9e74ab238e9a4a07ad         25        6          5   \n",
      "49663  fff4e8465d1e12621bc361276b6217cf         25        0          2   \n",
      "49664  fff4e8465d1e12621bc361276b6217cf         25        2          1   \n",
      "49665  fff4e8465d1e12621bc361276b6217cf         25        6          1   \n",
      "\n",
      "       temperature       pm25  \n",
      "0         6.548387  133.83871  \n",
      "1         6.548387  133.83871  \n",
      "2         6.548387  133.83871  \n",
      "3         6.548387  133.83871  \n",
      "4         6.548387  133.83871  \n",
      "...            ...        ...  \n",
      "49661     5.200000  126.40000  \n",
      "49662     5.200000  126.40000  \n",
      "49663     5.200000  126.40000  \n",
      "49664     5.200000  126.40000  \n",
      "49665     5.200000  126.40000  \n",
      "\n",
      "[49666 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# merge the orders data and weather data\n",
    "orders_weather_data = pd.merge(orders_data_grouped, weather_data_grouped, on='time_slot', how='inner')\n",
    "\n",
    "print(orders_weather_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        region_hash  region_id  poi_ids\n",
      "0  90c5a34f06ac86aee0fd70e2adce7d8a          1   653376\n",
      "1  f2c8c4bb99e6377d21de71275afd6cd2          2   343537\n",
      "2  58c7a4888306d8ff3a641d1c0feccbe3          3    31125\n",
      "3  b26a240205c852804ff8758628c0a86a          4   187829\n",
      "4  4b9e4cf2fbdc8281b8a1f9f12b80ce4d          5    27888\n"
     ]
    }
   ],
   "source": [
    "# merge the poi_list class characteristics with the cluster_map\n",
    "# cluster_map: region_hash, region_id\n",
    "# poi_data: district_hash, poi_ids\n",
    "# merge on district_hash\n",
    "cluster_map_poi = pd.merge(cluster_map, poi_data, left_on='region_hash', right_on='region_hash', how='inner')\n",
    "\n",
    "# remove the region_hash column\n",
    "# cluster_map_poi = cluster_map_poi.drop(['region_id'], axis=1)\n",
    "\n",
    "print(cluster_map_poi.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       time_slot  weekday  order_gap  temperature        pm25  region_id  \\\n",
      "0              0        0          1     6.548387  133.838710         50   \n",
      "1              0        3          1     6.548387  133.838710         50   \n",
      "2              0        4          5     6.548387  133.838710         50   \n",
      "3              0        5          2     6.548387  133.838710         50   \n",
      "4              0        6          3     6.548387  133.838710         50   \n",
      "...          ...      ...        ...          ...         ...        ...   \n",
      "44624        138        0          2     6.382353  128.941176         15   \n",
      "44625        139        0          4     6.387097  132.967742         15   \n",
      "44626        141        1          1     5.970588  127.941176         15   \n",
      "44627        142        0          1     6.085714  125.971429         15   \n",
      "44628        142        2          1     6.085714  125.971429         15   \n",
      "\n",
      "       poi_ids  \n",
      "0        57270  \n",
      "1        57270  \n",
      "2        57270  \n",
      "3        57270  \n",
      "4        57270  \n",
      "...        ...  \n",
      "44624    14442  \n",
      "44625    14442  \n",
      "44626    14442  \n",
      "44627    14442  \n",
      "44628    14442  \n",
      "\n",
      "[44629 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# merge the orders_data with the cluster_map_poi\n",
    "# orders_weather_data: start_district_hash, time_slot, weekday, order_gap, temperature, pm25\n",
    "# cluster_map_poi: region_id, poi_ids\n",
    "# merge on start_district_hash\n",
    "# print(orders_weather_data.head())\n",
    "# print(cluster_map_poi.head())\n",
    "\n",
    "orders_weather_cluster_map_poi = pd.merge(orders_weather_data, cluster_map_poi, left_on='start_district_hash', right_on='region_hash', how='inner')\n",
    "\n",
    "# remove the start_district_hash column\n",
    "orders_weather_cluster_map_poi = orders_weather_cluster_map_poi.drop(['start_district_hash', 'region_hash'], axis=1)\n",
    "\n",
    "print(orders_weather_cluster_map_poi)\n",
    "\n",
    "\n",
    "# save the data\n",
    "orders_weather_cluster_map_poi.to_csv('../dataset/processedData/orders_weather_cluster_map_poi.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
