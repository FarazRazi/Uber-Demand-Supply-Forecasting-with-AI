{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to clean and pre-process the data\n",
    "# import data using pandas\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_map finished\n",
      "cluster_map.head(): \n",
      "                         region_hash  region_id\n",
      "0  90c5a34f06ac86aee0fd70e2adce7d8a          1\n",
      "1  f2c8c4bb99e6377d21de71275afd6cd2          2\n",
      "2  58c7a4888306d8ff3a641d1c0feccbe3          3\n",
      "3  b26a240205c852804ff8758628c0a86a          4\n",
      "4  4b9e4cf2fbdc8281b8a1f9f12b80ce4d          5\n"
     ]
    }
   ],
   "source": [
    "# label the cluster map\n",
    "# labels:\n",
    "# region_hash, region_id\n",
    "\n",
    "columns = ['region_hash', 'region_id']\n",
    "# read the cluster map\n",
    "cluster_map = pd.read_csv('../dataset/test_set/cluster_map/cluster_map', sep='\\t', on_bad_lines='skip', header=None, names=columns)\n",
    "print('cluster_map finished')\n",
    "\n",
    "print('cluster_map.head(): \\n', cluster_map.head())\n",
    "\n",
    "\n",
    "cluster_map.to_csv('../dataset/labeledTestSet/cluster_map.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename:  ../dataset/test_set/order_data\\test_order_data_2016-01-23\n",
      "filename:  ../dataset/test_set/order_data\\test_order_data_2016-01-25\n",
      "filename:  ../dataset/test_set/order_data\\test_order_data_2016-01-27\n",
      "filename:  ../dataset/test_set/order_data\\test_order_data_2016-01-29\n",
      "filename:  ../dataset/test_set/order_data\\test_order_data_2016-01-31\n",
      "orders_data finished\n"
     ]
    }
   ],
   "source": [
    "# label the orders data\n",
    "# labels:\n",
    "# order_id, driver_id, passenger_id, start_district_hash, dest_district_hash, price, time\n",
    "columns = ['order_id', 'driver_id', 'start_region_hash', 'dest_region_hash', 'time']\n",
    "\n",
    "# read the orders data\n",
    "orders_data = []\n",
    "for f in glob.glob('../dataset/test_set/order_data/test_order_data_*'):\n",
    "    # file name\n",
    "    print('filename: ', f)\n",
    "    df = pd.read_csv(f, sep=',', on_bad_lines='skip', header=None, names=columns)\n",
    "    orders_data.append(df)\n",
    "\n",
    "print('orders_data finished')\n",
    "orders_data = pd.concat(orders_data,  ignore_index=True)\n",
    "\n",
    "# print('orders_data.head(): ', orders_data.head())\n",
    "orders_data.to_csv('../dataset/labeledTestSet/orders_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename:  ../dataset/test_set/weather_data\\weather_data_2016-01-23_test\n",
      "filename:  ../dataset/test_set/weather_data\\weather_data_2016-01-25_test\n",
      "filename:  ../dataset/test_set/weather_data\\weather_data_2016-01-27_test\n",
      "filename:  ../dataset/test_set/weather_data\\weather_data_2016-01-29_test\n",
      "filename:  ../dataset/test_set/weather_data\\weather_data_2016-01-31_test\n",
      "weather_data finished\n"
     ]
    }
   ],
   "source": [
    "# label the weather data\n",
    "# labels:\n",
    "# time, weather, temperature, pm25\n",
    "columns = ['time', 'weather', 'temperature', 'pm25']\n",
    "\n",
    "# print('weather_data.head(): \\n', weather_data.head())\n",
    "\n",
    "\n",
    "# # read the weather data\n",
    "weather_data = []\n",
    "for f in glob.glob('../dataset/test_set/weather_data/weather_data_*'):\n",
    "    # file name\n",
    "    print('filename: ', f)\n",
    "    df = pd.read_csv(f, sep='\\t', on_bad_lines='skip', header=None, names=columns)\n",
    "    weather_data.append(df)\n",
    "\n",
    "print('weather_data finished')\n",
    "weather_data = pd.concat(weather_data, ignore_index=True)\n",
    "\n",
    "\n",
    "weather_data.to_csv('../dataset/labeledTestSet/weather_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled_poi_data.head():                          region_hash  poi_ids\n",
      "0  74c1c25f4b283fa74a5514307b0d0278   878555\n",
      "1  08f5b445ec6b29deba62e6fd8b0325a6    51128\n",
      "2  4b7f6f4e2bf237b6cc58f57142bea5c0   104248\n",
      "3  a814069db8d32f0fa6e188f41059c6e1   121844\n",
      "4  8316146a6f78cc6d9f113f0390859417    37599\n"
     ]
    }
   ],
   "source": [
    "# label the poi data\n",
    "# labels:\n",
    "# region_hash, poi_id \n",
    "# 1st column: district_hash\n",
    "# whole next column is: poi_id\n",
    "columns = ['region_hash', 'poi_id']\n",
    "\n",
    "\n",
    "\n",
    "# read the poi data\n",
    "poi_data = pd.read_csv('../dataset/test_set/poi_data/poi_data', sep='\\t', header=None, on_bad_lines='skip')\n",
    "\n",
    "# extract the district_hash column and the POI ID columns\n",
    "district_hash = poi_data.iloc[:, 0]\n",
    "poi_ids = poi_data.iloc[:, 1:]\n",
    "\n",
    "# combine all the POI IDs for each row into a list\n",
    "poi_ids_list = poi_ids.apply(lambda x: x.tolist(), axis=1)\n",
    "\n",
    "# combine the district_hash and poi_ids_list into a new DataFrame\n",
    "labeled_poi_data = pd.concat([district_hash, poi_ids_list], axis=1)\n",
    "labeled_poi_data.columns = ['region_hash', 'poi_ids']\n",
    "\n",
    "# print the result\n",
    "# print(labeled_poi_data.head())\n",
    "\n",
    "# updated list\n",
    "updated_list = []\n",
    "\n",
    "# convert the column of lists to a list of lists\n",
    "list_of_lists_poi_id = labeled_poi_data['poi_ids'].tolist()\n",
    "\n",
    "# poi format poi_id = class:numofFacilities\n",
    "# seperate numofFacilities from list_of_lists_poi_id and sum them up\n",
    "\n",
    "# for each list in list_of_lists_poi_id \n",
    "# change the list of poi_id to sum of numofFacilities \n",
    "for list in list_of_lists_poi_id:\n",
    "    sum = 0\n",
    "    for poi in list:\n",
    "        if(pd.isna(poi)==False):\n",
    "            poi = poi.split(':')\n",
    "            poi[1] = int(poi[1])\n",
    "            sum += poi[1]\n",
    "    updated_list.append(sum)\n",
    "\n",
    "# print(list_of_lists_poi_id)\n",
    "\n",
    "# change labeled_poi_data['poi_ids'] to list_of_lists_poi_id\n",
    "labeled_poi_data['poi_ids'] = updated_list\n",
    "\n",
    "print('labeled_poi_data.head(): ', labeled_poi_data.head())\n",
    "\n",
    "labeled_poi_data.to_csv('../dataset/labeledTestSet/poi_data.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from the labeled data\n",
    "cluster_map = pd.read_csv('../dataset/labeledTestSet/cluster_map.csv')\n",
    "orders_data = pd.read_csv('../dataset/labeledTestSet/orders_data.csv')\n",
    "weather_data = pd.read_csv('../dataset/labeledTestSet/weather_data.csv')\n",
    "poi_data = pd.read_csv('../dataset/labeledTestSet/poi_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           order_id                         driver_id  \\\n",
      "0  a903b5f7f65f1dc7f4ee94fec74673be  dce1e90fc91ed39a7b04a22d02910a7d   \n",
      "1  b7c838beaf12a2132776a1f00e016038  1f39eba5ce330f20c95177826a122e12   \n",
      "2  0e0d3c93298ed59281352e34c6f1ec5a  8e4e2bc0342b3c55edea2723f6613e36   \n",
      "3  1f6d0d7f68f216c4333969d6152a0a8b  5a33393e516673c8e9a065915667a30f   \n",
      "4  bcf6050d9f5b270f8beb3ff80b01b435  f0232b8e45abc5ca92ca0f90fa811e7c   \n",
      "\n",
      "                  start_region_hash                  dest_region_hash  \\\n",
      "0  d4ec2125aff74eded207d2d915ef682f  d4ec2125aff74eded207d2d915ef682f   \n",
      "1  2407d482f0ffa22a947068f2551fe62c  2407d482f0ffa22a947068f2551fe62c   \n",
      "2  b26a240205c852804ff8758628c0a86a  3a43dcdff3c0b66b1acb1644ff055f9d   \n",
      "3  4725c39a5e5f4c188d382da3910b3f3f  4725c39a5e5f4c188d382da3910b3f3f   \n",
      "4  dd8d3b9665536d6e05b29c2648c0e69a  a5609739c6b5c2719a3752327c5e33a7   \n",
      "\n",
      "   time_slot  weekday  \n",
      "0         44        5  \n",
      "1         78        5  \n",
      "2         92        5  \n",
      "3        102        5  \n",
      "4        114        5  \n",
      "   weather  temperature  pm25  time_slot  weekday\n",
      "0        4          1.0    94         42        5\n",
      "1        3         -1.0   107         67        5\n",
      "2        3         -1.0    56         79        5\n",
      "3        3          0.0    48         90        5\n",
      "4        3          0.0    48         91        5\n"
     ]
    }
   ],
   "source": [
    "# map time to time slot\n",
    "# devide day in 10 min time slots (144 time slots)\n",
    "\n",
    "\n",
    "# convert time to datetime\n",
    "orders_data['time'] = pd.to_datetime(orders_data['time'])\n",
    "weather_data['time'] = pd.to_datetime(weather_data['time'])\n",
    "\n",
    "# map time to time slot\n",
    "orders_data['time_slot'] = orders_data['time'].dt.hour * 6 + orders_data['time'].dt.minute // 10\n",
    "weather_data['time_slot'] = weather_data['time'].dt.hour * 6 + weather_data['time'].dt.minute // 10\n",
    "\n",
    "# map time to time slot with weekday\n",
    "orders_data['weekday'] = orders_data['time'].dt.weekday\n",
    "weather_data['weekday'] = weather_data['time'].dt.weekday\n",
    "\n",
    "# remove the time column\n",
    "orders_data = orders_data.drop(['time'], axis=1)\n",
    "weather_data = weather_data.drop(['time'], axis=1)\n",
    "\n",
    "print(orders_data.head())\n",
    "print(weather_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     start_region_hash  time_slot  weekday  supply\n",
      "0     08232402614a9b48895cc3d0aeb0e9f2         43        2       4\n",
      "1     08232402614a9b48895cc3d0aeb0e9f2         43        5       1\n",
      "2     08232402614a9b48895cc3d0aeb0e9f2         44        2       1\n",
      "3     08232402614a9b48895cc3d0aeb0e9f2         44        5       3\n",
      "4     08232402614a9b48895cc3d0aeb0e9f2         54        0       1\n",
      "...                                ...        ...      ...     ...\n",
      "7885  fff4e8465d1e12621bc361276b6217cf        138        5       4\n",
      "7886  fff4e8465d1e12621bc361276b6217cf        138        6       1\n",
      "7887  fff4e8465d1e12621bc361276b6217cf        139        5       1\n",
      "7888  fff4e8465d1e12621bc361276b6217cf        140        0       2\n",
      "7889  fff4e8465d1e12621bc361276b6217cf        140        4       1\n",
      "\n",
      "[7890 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# group the orders data by time slot \n",
    "# aggregate count the number of orders where driver_id = NULL\n",
    "# this is supply demand deficit\n",
    "\n",
    "orders_data_grouped = orders_data.groupby(['start_region_hash','time_slot', 'weekday' ]).agg({'order_id': 'count'}).rename(columns={'order_id': 'supply'}).reset_index()\n",
    "\n",
    "print(orders_data_grouped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    time_slot  temperature       pm25\n",
      "0          42     2.000000  81.333333\n",
      "1          43     2.500000  75.000000\n",
      "2          44     2.500000  75.000000\n",
      "3          54     6.000000  30.000000\n",
      "4          55     0.000000  65.000000\n",
      "5          56     3.000000  73.000000\n",
      "6          66     3.500000  74.500000\n",
      "7          67     3.000000  71.250000\n",
      "8          68     1.500000  61.000000\n",
      "9          78     3.500000  63.250000\n",
      "10         79     2.000000  66.500000\n",
      "11         80     3.500000  63.250000\n",
      "12         90     3.250000  50.250000\n",
      "13         91     3.000000  52.333333\n",
      "14         92     4.000000  65.500000\n",
      "15        102     4.000000  58.333333\n",
      "16        103     1.666667  78.333333\n",
      "17        104     3.666667  58.333333\n",
      "18        114     2.500000  74.000000\n",
      "19        115     2.000000  66.000000\n",
      "20        116     2.000000  62.000000\n",
      "21        126     1.333333  84.333333\n",
      "22        127     0.800000  70.000000\n",
      "23        128     2.000000  74.750000\n",
      "24        138     0.800000  73.800000\n",
      "25        139     0.800000  73.800000\n",
      "26        140     0.800000  73.800000\n"
     ]
    }
   ],
   "source": [
    "# group the weather data by time slot\n",
    "# aggregate the mean of temperature and pm25\n",
    "weather_data_grouped = weather_data.groupby(['time_slot']).agg({'temperature': 'mean', 'pm25': 'mean'}).reset_index()\n",
    "\n",
    "print(weather_data_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     start_region_hash  time_slot  weekday  supply  \\\n",
      "0     08232402614a9b48895cc3d0aeb0e9f2         43        2       4   \n",
      "1     08232402614a9b48895cc3d0aeb0e9f2         43        5       1   \n",
      "2     08f5b445ec6b29deba62e6fd8b0325a6         43        5       1   \n",
      "3     08f5b445ec6b29deba62e6fd8b0325a6         43        6       1   \n",
      "4     1afd7afbc81ecc1b13886a569d869e8a         43        2     124   \n",
      "...                                ...        ...      ...     ...   \n",
      "7885  fc34648599753c9e74ab238e9a4a07ad         42        5      18   \n",
      "7886  fc34648599753c9e74ab238e9a4a07ad         42        6      23   \n",
      "7887  fff4e8465d1e12621bc361276b6217cf         42        2       1   \n",
      "7888  fff4e8465d1e12621bc361276b6217cf         42        5       2   \n",
      "7889  fff4e8465d1e12621bc361276b6217cf         42        6       3   \n",
      "\n",
      "      temperature       pm25  \n",
      "0             2.5  75.000000  \n",
      "1             2.5  75.000000  \n",
      "2             2.5  75.000000  \n",
      "3             2.5  75.000000  \n",
      "4             2.5  75.000000  \n",
      "...           ...        ...  \n",
      "7885          2.0  81.333333  \n",
      "7886          2.0  81.333333  \n",
      "7887          2.0  81.333333  \n",
      "7888          2.0  81.333333  \n",
      "7889          2.0  81.333333  \n",
      "\n",
      "[7890 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# merge the orders data and weather data\n",
    "orders_weather_data = pd.merge(orders_data_grouped, weather_data_grouped, on='time_slot', how='inner')\n",
    "\n",
    "print(orders_weather_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        region_hash  region_id  poi_ids\n",
      "0  90c5a34f06ac86aee0fd70e2adce7d8a          1   653376\n",
      "1  f2c8c4bb99e6377d21de71275afd6cd2          2   343537\n",
      "2  58c7a4888306d8ff3a641d1c0feccbe3          3    31125\n",
      "3  b26a240205c852804ff8758628c0a86a          4   187829\n",
      "4  4b9e4cf2fbdc8281b8a1f9f12b80ce4d          5    27888\n"
     ]
    }
   ],
   "source": [
    "# merge the poi_list class characteristics with the cluster_map\n",
    "# cluster_map: region_hash, region_id\n",
    "# poi_data: district_hash, poi_ids\n",
    "# merge on district_hash\n",
    "cluster_map_poi = pd.merge(cluster_map, poi_data, left_on='region_hash', right_on='region_hash', how='inner')\n",
    "\n",
    "# remove the region_hash column\n",
    "# cluster_map_poi = cluster_map_poi.drop(['region_id'], axis=1)\n",
    "\n",
    "print(cluster_map_poi.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      time_slot  weekday  supply  temperature       pm25  region_id  poi_ids\n",
      "0            43        2       4     2.500000  75.000000         50    57270\n",
      "1            43        5       1     2.500000  75.000000         50    57270\n",
      "2            44        2       1     2.500000  75.000000         50    57270\n",
      "3            44        5       3     2.500000  75.000000         50    57270\n",
      "4            54        0       1     6.000000  30.000000         50    57270\n",
      "...         ...      ...     ...          ...        ...        ...      ...\n",
      "7240        115        6       1     2.000000  66.000000         15    14442\n",
      "7241        126        2       3     1.333333  84.333333         15    14442\n",
      "7242        126        6       2     1.333333  84.333333         15    14442\n",
      "7243        127        2       1     0.800000  70.000000         15    14442\n",
      "7244        128        4       1     2.000000  74.750000         15    14442\n",
      "\n",
      "[7245 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# merge the orders_data with the cluster_map_poi\n",
    "# orders_weather_data: start_district_hash, time_slot, weekday, order_gap, temperature, pm25\n",
    "# cluster_map_poi: region_id, poi_ids\n",
    "# merge on start_district_hash\n",
    "# print(orders_weather_data.head())\n",
    "# print(cluster_map_poi.head())\n",
    "\n",
    "orders_weather_cluster_map_poi = pd.merge(orders_weather_data, cluster_map_poi, left_on='start_region_hash', right_on='region_hash', how='inner')\n",
    "\n",
    "# remove the start_district_hash column\n",
    "orders_weather_cluster_map_poi = orders_weather_cluster_map_poi.drop(['start_region_hash', 'region_hash'], axis=1)\n",
    "\n",
    "print(orders_weather_cluster_map_poi)\n",
    "\n",
    "\n",
    "# save the data\n",
    "orders_weather_cluster_map_poi.to_csv('../dataset/processedData/orders_weather_cluster_map_poi_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
